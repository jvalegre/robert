#####################################################.
#     This file stores functions from REPORT        #
#####################################################.

import os
import pandas as pd
import textwrap
from pathlib import Path


def get_csv_names(command_line,dat_files):
    """
    Detects the name used in the csv_name option from a command line or manual input
    """
    
    csv_name = ''
    if 'csv_name' in command_line:
        csv_name = command_line.split('csv_name')[1].split()[0]
        if csv_name[0] in ['"',"'"]:
            csv_name = csv_name[1:]
        if csv_name[-1] in ['"',"'"]:
            csv_name = csv_name[:-1]
    else:
        for module in ['CURATE','GENERATE']:
            if module in dat_files:
                for line in dat_files[module]:
                    if 'csv_name option set to ' in line:
                        csv_name = line.split('csv_name option set to ')[1].split()[0]
                        break
                if csv_name != '':
                    break
    
    csv_test = ''
    if 'csv_test' in command_line:
        csv_test = command_line.split('csv_test')[1].split()[0]
        if csv_test[0] in ['"',"'"]:
            csv_test = csv_test[1:]
        if csv_test[-1] in ['"',"'"]:
            csv_test = csv_test[:-1]
    
    return csv_name,csv_test


def get_images(module):
    """
    Generate the string that includes images
    """
    
    module_path = Path(f'{os.getcwd()}/{module}')
    module_images = [str(file_path) for file_path in module_path.rglob('*.png')]

    if module not in ['PREDICT','VERIFY']:
        image_caption = f'<br>------- Images generated by the {module} module -------'
    else:
        image_caption = f'<br>------- Images and summary generated by the {module} module -------'
        module_file = f'{os.getcwd()}/{module}/{module}_data.dat'
        columns_score = []
        for suffix in ['No PFI','PFI']:
            columns_score.append(get_summary(module,module_file,suffix))
        
        # Combine both columns
        image_caption += combine_cols(columns_score)


    if module == 'VERIFY':
        if len(module_images) == 2 and 'No_PFI' in module_images[1]:
            module_images = revert_list(module_images)

    if module == 'PREDICT':
        results_images = []
        shap_images = []
        pfi_images = []
        outliers_images = []

        for image_path in module_images:
            filename = Path(image_path).stem
            if "_REPORT" not in filename:
                if "Results" in filename:
                    results_images.append(image_path)
                elif "SHAP" in filename:
                    shap_images.append(image_path)
                elif "Outliers" in filename:
                    outliers_images.append(image_path)
                else:
                    pfi_images.append(image_path)

        # keep the ordering (No_PFI in the left, PFI in the right of the PDF)
        if len(results_images) == 2 and 'No_PFI' in results_images[1]:
            results_images = revert_list(results_images)
        if len(shap_images) == 2 and 'No_PFI' in shap_images[1]:
            shap_images = revert_list(shap_images)
        if len(pfi_images) == 2 and 'No_PFI' in pfi_images[1]:
            pfi_images = revert_list(pfi_images)
        if len(outliers_images) == 2 and 'No_PFI' in outliers_images[1]:
            outliers_images = revert_list(outliers_images)
        
        html_png = ''
        for _,image_pair in enumerate([results_images, shap_images, pfi_images, outliers_images]):
            
            pair_list = ''.join([f'<img src="file:///{image_path}" style="margin-bottom: 10px; margin-top: 10px; margin-left: -13px; width: 100%, margin: 0"/>' for image_path in image_pair])
            html_png += f'<pre style="text-align: center;">{pair_list}</pre>'

    if module != 'PREDICT':
        pair_list = ''.join([f'<img src="file:///{image_path}" style="margin-bottom: 10px; margin-top: 10px; margin-left: -13px; width: 100%, margin: 0"/>' for image_path in module_images])
        html_png = f'<pre style="text-align: center;">{pair_list}</pre>'

    imag_lines = f"""
<p style="text-align: center; margin: 0;"><span style="font-weight:bold;">
{image_caption}
</span></p>
{html_png} 
"""
    
    return imag_lines


def get_summary(module,file,suffix,titles=True):
    """
    Retrieve the summary of results from the PREDICT and VERIFY dat files
    """
    
    with open(file, 'r') as datfile:
        lines = datfile.readlines()
        start_results,stop_results = 0,0
        train_outliers,valid_outliers,test_outliers = [],[],[]
        for i,line in enumerate(lines):
            if suffix == 'No PFI':
                if module == 'PREDICT':
                    if 'o  Results saved in' in line and 'No_PFI.dat' in line:
                        start_results,stop_results = locate_results(i,lines)
                    elif 'o  Outlier values saved' in line and 'No_PFI.dat' in line:
                        train_outliers,valid_outliers,test_outliers = locate_outliers(i,lines)
            if suffix == 'PFI':
                if module == 'PREDICT':
                    if 'o  Results saved in' in line and 'No_PFI.dat' not in line:
                        start_results,stop_results = locate_results(i,lines)
                    elif 'o  Outlier values saved' in line and 'No_PFI.dat' not in line:
                        train_outliers,valid_outliers,test_outliers = locate_outliers(i,lines)

        # add the summary of results of PREDICT
        # for the SCORE section, only results
        if not titles:
            start_results += 4
            summary = []
        else:
            summary = ['<i>Prediction metrics and descriptors</i>\n\n']
        for line in lines[start_results:stop_results+1]:
            if 'R2' in line:
                line = line.replace('R2','R<sup>2</sup>')
            if titles:
                summary.append(line[6:])

            else: # for the SCORE section, only results
                if suffix == 'No PFI':
                    summary.append(line[8:])
                elif suffix == 'PFI':
                    summary.append(f'   {line[8:]}')

        # add the outlier part
        if titles: # only add this to the PREDICT section, not to the SCORE section
            summary.append('\n<i>Outliers (max. 10 shown)</i>\n\n')
            summary = summary + train_outliers + valid_outliers + test_outliers

        summary = ''.join(summary)

    # Column 1: No PFI
    if titles:
        if suffix == 'No PFI':
            column = f"""
            <p><span style="font-weight:bold;">No PFI (all descriptors):</span></p>
            <pre style="text-align: justify;">{summary}</pre>
            """

        # Column 2: PFI
        elif suffix == 'PFI':
            if titles:
                column = f"""
                <p><span style="font-weight:bold;">PFI (only important descriptors):</span></p>
                <pre style="text-align: justify;">{summary}</pre>
                """
    
    else:
        column = f"""
        <pre style="text-align: justify;">{summary}</pre>
        """

    return column


def locate_outliers(i,lines):
    """
    Returns the start and end of the PREDICT summary in the dat file
    """
    
    train_outliers,valid_outliers,test_outliers = [],[],[]
    for j in range(i+1,len(lines)):
        if 'Train:' in lines[j]:
            for k in range(j,len(lines)):
                if 'Validation:' in lines[k]:
                    break
                elif len(train_outliers) <= 10: # 10 outliers and the line with the % of outliers
                    train_outliers.append(lines[k][6:])
        elif 'Validation:' in lines[j]:
            for k in range(j,len(lines)):
                if 'Test:' in lines[k] or len(lines[k].split()) == 0:
                    break
                elif len(valid_outliers) <= 10: # 10 outliers and the line with the % of outliers
                    valid_outliers.append(lines[k][6:])
        elif 'Test:' in lines[j]:
            for k in range(j,len(lines)):
                if len(lines[k].split()) == 0:
                    break
                elif len(test_outliers) <= 10: # 10 outliers and the line with the % of outliers
                    test_outliers.append(lines[k][6:])

        if len(lines[j].split()) == 0:
            break

    return train_outliers,valid_outliers,test_outliers


def locate_results(i,lines):
    """
    Returns the start and end of the outliers section in the PREDICT dat file
    """
    
    start_results = i+1
    stop_results = i+6
    for j in range(i+1,i+10):
        if '-  Test : ' in lines[j]:
            stop_results = i+7
    
    return start_results,stop_results

   
def combine_cols(columns):
    """
    Makes a string with multi-column lines
    """
    
    column_data = ''
    for column in columns:
        column_data += f'<div style="flex: 1;">{column}</div>'

    combined_data = f"""
    <div style="display: flex;">
    {column_data}
    </div>
    """

    return combined_data


def revert_list(list_tuple):
    """
    Reverts the order of a list of two components
    """

    new_sort = [] # for some reason reverse() gives a weird issue when reverting lists
    new_sort.append(list_tuple[1])
    new_sort.append(list_tuple[0])

    return new_sort


def get_time(file):
    """
    Returns the execution time of the modules
    """

    module_time = 'x  ROBERT did not save any execution time for this module'
    with open(file, 'r') as datfile:
        for line in reversed(datfile.readlines()):
            if 'Time' in line and 'seconds' in line:
                module_time = line
                break

    return module_time


def get_col_score(score_info,data_score,suffix,csv_test,spacing_PFI):
    """
    Gather the information regarding the score of the No PFI and PFI models
    """
    
    r2_pts = get_pts(data_score['r2_score'])
    outliers_pts = get_pts(data_score['outliers_score'])
    descp_pts = get_pts(data_score['descp_score'])
    verify_pts = get_pts(data_score['verify_score'])

    spacing_r2 = get_spacing(data_score['r2_score'])
    spacing_outliers = get_spacing(data_score['outliers_score'])
    spacing_descp = get_spacing(data_score['descp_score'])
    spacing_verify = get_spacing(data_score['verify_score'])

    first_line = f'<p style="text-align: justify; margin-top: -8px;">{spacing_PFI}' # reduces line separation separation
    reduced_line = f'<p style="text-align: justify; margin-top: -10px;">{spacing_PFI}' # reduces line separation separation        

    if csv_test != '':
        score_set = 'test'
    else:
        score_set = 'valid.'
        
    ML_line_format = f'<p style="text-align: justify; margin-top: -2px; margin-bottom: 0px;">{spacing_PFI}'
    part_line_format = f'<p style="text-align: justify; margin-top: 0px; margin-bottom: 4px;">{spacing_PFI}'

    if suffix == 'No PFI':
        caption = f'{spacing_PFI}No PFI (all descriptors):'

    elif suffix == 'PFI':
        caption = f'{spacing_PFI}PFI (only important descriptors):'

    score_info += f"""{first_line}{r2_pts}{spacing_r2} The {score_set} set shows an R<sup>2</sup> of {data_score['r2_valid']}</p>
{reduced_line}{outliers_pts}{spacing_outliers} The {score_set} set has {data_score['outliers_prop']}% of outliers</p>
{reduced_line}{descp_pts}{spacing_descp} The {score_set} set uses {data_score['proportion_ratio']} points:descriptors</p>
{reduced_line}{verify_pts}{spacing_verify} The {score_set} set passes {data_score['verify_score']} VERIFY tests</p>
"""

    partitions_ratio = data_score['proportion_ratio_print'].split('-  ')[1]
    
    column = f"""<p style="margin-top:-12px;"><span style="font-weight:bold;">{caption}</span></p>
    {ML_line_format}ML model: {data_score['ML_model']}</p>
    {part_line_format}{partitions_ratio}</p>
    {score_info}
    <p style="margin-bottom: 20px;"></p>
    """

    return column


def get_col_text(type_thres):
    """
    Gather the information regarding the thresholds used in the score and abbreviation sections
    """

    first_line = '<p style="text-align: justify; margin-top: -8px;">' # reduces line separation separation
    reduced_line = '<p style="text-align: justify; margin-top: -10px;">' # reduces line separation separation
    if type_thres == 'R<sup>2</sup>':
        column = f"""<pre style="text-align: justify;"><span style="font-weight:bold;">R<sup>2</sup>  <u>                           </u></span></pre>
{first_line}{get_pts(2)}&nbsp;&nbsp;  R<sup>2</sup> > 0.85</p>
{reduced_line}{get_pts(1)}&nbsp;&nbsp;&nbsp;&nbsp;    0.85 > R<sup>2</sup> > 0.70</p>
{reduced_line}{get_pts(0)}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     R<sup>2</sup> < 0.70</pre></p>
"""

    elif type_thres == 'outliers':
        column = f"""<pre style="text-align: justify;"><span style="font-weight:bold;">Outliers  <u>                          </u></span></pre>
{first_line}{get_pts(2)}&nbsp;&nbsp;  < 7.5% of outliers</p>
{reduced_line}{get_pts(1)}&nbsp;&nbsp;&nbsp;&nbsp;    7.5% < outliers < 15%</p>
{reduced_line}{get_pts(0)}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     > 15% of outliers</p>
"""

    elif type_thres == 'descps':
        column = f"""<pre style="text-align: justify;"><span style="font-weight:bold;">    Points:descriptors  <u>      </u></span></pre>
{first_line}&nbsp;&nbsp;&nbsp;&nbsp;{get_pts(2)}&nbsp;&nbsp;  > 10:1 p:d ratio</p>
{reduced_line}&nbsp;&nbsp;&nbsp;&nbsp;{get_pts(1)}&nbsp;&nbsp;&nbsp;&nbsp;    10:1 > p:d ratio > 3:1</p>
{reduced_line}&nbsp;&nbsp;&nbsp;&nbsp;{get_pts(0)}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     p:d ratio < 3:1</p>
"""

    elif type_thres == 'VERIFY':
        column = f"""<pre style="text-align: justify;"><span style="font-weight:bold;">        VERIFY tests  <u>             </u></span></pre>
{first_line}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Up to {get_pts(4)} (tests pass)</p>
{reduced_line}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{get_pts(0)}&nbsp; (all tests failed)</pre></p>
"""

    elif type_thres in ['abbrev_1','abbrev_2']:
        first_line = '<p style="text-align: justify; margin-top: -13px;">'
        if type_thres == 'abbrev_1':
            abbrev_list = ['<strong>ACC:</strong> accuracy',
                           '<strong>ADAB:</strong> AdaBoost',
                            '<strong>CSV:</strong> comma separated values',
                            '<strong>CLAS:</strong> classification',
                            '<strong>CV:</strong> cross-validation',
                            '<strong>F1 score:</strong> balanced F-score',
                            '<strong>GB:</strong> gradient boosting',
                            '<strong>GP:</strong> gaussian process',
                            '<strong>KN:</strong> k-nearest neighbors',
                            '<strong>MAE:</strong> root-mean-square error', 
                            "<strong>MCC:</strong> Matthew's correlation coefficient",
            ]
        elif type_thres == 'abbrev_2':
            abbrev_list = ['<strong>ML:</strong> machine learning',                          
                            '<strong>MVL:</strong> multivariate lineal models',
                            '<strong>NN:</strong> neural network',
                            '<strong>PFI:</strong> permutation feature importance',
                            '<strong>R2:</strong> coefficient of determination',
                            '<strong>REG:</strong> Regression',
                            '<strong>RF:</strong> random forest',
                            '<strong>RMSE:</strong> root mean square error',
                            '<strong>RND:</strong> random',
                            '<strong>SHAP:</strong> Shapley additive explanations',
                            '<strong>VR:</strong> voting regressor',
            ]

        column = ''
        for i,ele in enumerate(abbrev_list):
            if i == 0:
                column += f"""{first_line}{ele}</p>
"""
            else:
                column += f"""{reduced_line}{ele}</p>
"""

    return column


def get_col_transpa(params_dict,suffix,section):
    """
    Gather the information regarding the model parameters represented in the Reproducibility section
    """

    first_line = '<p style="text-align: justify; margin-top: -40px;">' # reduces line separation separation
    reduced_line = '<p style="text-align: justify; margin-top: -8px;">' # reduces line separation separation

    if suffix == 'No_PFI':
        caption = f'No PFI (all descriptors):'

    elif suffix == 'PFI':
        caption = f'PFI (only important descriptors):'

    excluded_params = [params_dict['error_type'],'train','y']
    misc_params = ['split','type','error_type']
    if params_dict['type'] == 'reg':
        model_type = 'Regressor'
    elif params_dict['type'] == 'clas':
        model_type = 'Classifier'
    models_dict = {'RF': f'RandomForest{model_type}',
                    'MVL': 'LinearRegression',
                    'GB': f'GradientBoosting{model_type}',
                    'NN': f'MLP{model_type}',
                    'GP': f'GaussianProcess{model_type}',
                    'ADAB': f'AdaBoost{model_type}',
                    'VR': f'Voting{model_type} (combining RF, GB and NN)'
                    }

    col_info,sklearn_model = '',''
    for _,ele in enumerate(params_dict.keys()):
        if ele not in excluded_params:
            if ele == 'model' and section == 'model_section':
                sklearn_model = models_dict[params_dict[ele].upper()]
                sklearn_model = f"""{first_line}sklearn model: {sklearn_model}</p>"""
            elif section == 'model_section' and ele.lower() not in misc_params:
                if ele != 'X_descriptors':
                    if ele == 'seed':
                        col_info += f"""{reduced_line}random_state: {params_dict[ele]}</p>"""
                    else:
                        col_info += f"""{reduced_line}{ele}: {params_dict[ele]}</p>"""
            elif section == 'misc_section' and ele.lower() in misc_params:
                if col_info == '':
                    col_info += f"""{first_line}{ele}: {params_dict[ele]}</p>"""
                else:
                    col_info += f"""{reduced_line}{ele}: {params_dict[ele]}</p>"""
    
    column = f"""<p style="margin-top: -30px;"><span style="font-weight:bold;">{caption}</span></p>
    {sklearn_model}{col_info}
    """

    return column


def get_pts(score):
    """
    Get a string with the numbers of points or lines to add
    """

    if score > 0:
        str_pts = f"{score*'&#9679;'}"
    else:
        str_pts = f"-"

    return str_pts


def get_spacing(score):
    """
    Get a string with the number of spaces needed
    """

    if score == 4:
        spacing = f"&nbsp;"
    if score == 3:
        spacing = f"&nbsp;&nbsp;&nbsp;"
    if score == 2:
        spacing = f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"
    if score == 1:
        spacing = f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"
    if score == 0:
        spacing = f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"

    return spacing


def get_verify_scores(dat_verify,suffix):
    """
    Calculates scores that come from the VERIFY module (VERIFY tests)
    """

    start_data = False
    verify_score = 0
    for i,line in enumerate(dat_verify):
        # set starting points for No PFI and PFI models
        if suffix == 'No PFI':
            if '------- ' in line and '(No PFI)' in line:
                start_data = True
            elif '------- ' in line and 'with PFI' in line:
                start_data = False
        if suffix == 'PFI':
            if '------- ' in line and 'with PFI' in line:
                start_data = True
        
        if start_data:
            if 'Original ' in line and '(validation set) =' in line:
                for j in range(i+1,i+5):
                    if 'PASSED' in dat_verify[j]:
                        verify_score += 1

    return verify_score


def get_predict_scores(dat_predict,suffix):
    """
    Calculates scores that come from the PREDICT module (R2, datapoints:descriptors ratio, outlier proportion)
    """

    start_data, test_set = False, False
    data_score = {}
    data_score['r2_score'] = 0
    data_score['descp_score'] = 0
    data_score['outliers_score'] = 0

    for i,line in enumerate(dat_predict):

        # set starting points for No PFI and PFI models
        if suffix == 'No PFI':
            if '------- ' in line and '(No PFI)' in line:
                start_data = True
            elif '------- ' in line and 'with PFI' in line:
                start_data = False
        if suffix == 'PFI':
            if '------- ' in line and 'with PFI' in line:
                start_data = True
        
        if start_data:
            # R2 and proportion
            if 'o  Results saved in PREDICT/' in line:
                data_score['ML_model'] = line.split('_')[1]
                data_score['proportion_ratio_print'] = dat_predict[i+2]
                # R2 from test (if any) or validation
                if '-  Test : R2' in dat_predict[i+7]:
                    data_score['r2_valid'] = float(dat_predict[i+7].split()[5].split(',')[0])
                    test_set = True
                elif '-  Validation : R2' in dat_predict[i+6]:
                    data_score['r2_valid'] = float(dat_predict[i+6].split()[5].split(',')[0])
                if data_score['r2_valid'] > 0.85:
                    data_score['r2_score'] += 2
                elif data_score['r2_valid'] > 0.7:
                    data_score['r2_score'] += 1

                # proportion
                data_score['proportion_ratio'] = dat_predict[i+4].split()[-1]
                proportion = int(data_score['proportion_ratio'].split(':')[0]) / int(data_score['proportion_ratio'].split(':')[1])
                if proportion > 10:
                    data_score['descp_score'] += 2
                elif proportion > 3:
                    data_score['descp_score'] += 1

            # outliers
            if 'o  Outlier values saved in' in line:
                for j in range(i,len(dat_predict)):
                    if test_set and 'Test:' in dat_predict[j]:
                        outliers_prop = dat_predict[j].split()[-1]
                        outliers_prop = outliers_prop.split('%)')[0]
                        data_score['outliers_prop'] = float(outliers_prop.split('(')[-1])
                    elif not test_set and 'Validation:' in dat_predict[j]:
                        outliers_prop = dat_predict[j].split()[-1]
                        outliers_prop = outliers_prop.split('%)')[0]
                        data_score['outliers_prop'] = float(outliers_prop.split('(')[-1])
                    elif len(dat_predict[j].split()) == 0:
                        break
                if data_score['outliers_prop'] < 7.5:
                    data_score['outliers_score'] += 2
                elif data_score['outliers_prop'] < 15:
                    data_score['outliers_score'] += 1

    return data_score


def repro_info(modules):
    """
    Retrieves variables used in the Reproducibility and Transparency section
    """

    version_n_date, citation, command_line = '','',''
    python_version, intelex_version, total_time = '','',0
    intelex_installed = True
    dat_files = {}
    for module in modules:
        path_file = Path(f'{os.getcwd()}/{module}/{module}_data.dat')
        if os.path.exists(path_file):
            datfile = open(path_file, 'r', errors="replace")
            txt_file = []
            for line in datfile:
                txt_file.append(line)
                if module.upper() in ['GENERATE','VERIFY','PREDICT']:
                    if 'The scikit-learn-intelex accelerator is not installed' in line:
                        intelex_installed = False
                if 'Time' in line and 'seconds' in line:
                    total_time += float(line.split()[2])
                if 'How to cite: ' in line:
                    citation = line.split('How to cite: ')[1]
                if 'ROBERT v' == line[:8]:
                    version_n_date = line
                if 'Command line used in ROBERT: ' in line:
                    command_line = line.split('Command line used in ROBERT: ')[1]
            total_time = round(total_time,2)
            dat_files[module] = txt_file
            datfile.close()
 
    try:
        import platform
        python_version = platform.python_version()
    except:
        python_version = '(version could not be determined)'
    if intelex_installed:
        try:
            import pkg_resources
            intelex_version = pkg_resources.get_distribution("scikit-learn-intelex").version
        except:
            intelex_version = '(version could not be determined)'
    else:
        intelex_version = 'not installed'
    
    return version_n_date, citation, command_line, python_version, intelex_version, total_time, dat_files


def make_report(report_html, HTML):
    """
    Generate a css file that will be used to make the PDF file
    """

    css_files = ["report.css"]
    outfile = f"{os.getcwd()}/ROBERT_report.pdf"
    if os.path.exists(outfile):
        os.remove(outfile)
    pdf = make_pdf(report_html, HTML, css_files)
    _ = Path(outfile).write_bytes(pdf)


def make_pdf(html, HTML, css_files):
    """Generate a PDF file from a string of HTML"""
    htmldoc = HTML(string=html, base_url="")
    if css_files:
        htmldoc = htmldoc.write_pdf(stylesheets=css_files)
    else:
        htmldoc = htmldoc.write_pdf()
    return htmldoc


def css_content(csv_name,robert_version):
    """
    Obtain ROBERT version and CSV name to use it on top of the PDF report
    """

    css_content = f"""
    body {{
    font-size: 12px;
    line-height: 1.5;
    }}
    @page {{
        size: A4;
        margin: 2cm;
        @bottom-right {{
            content: "Page "counter(page) " of " counter(pages);
            font-size: 8pt;
            position: fixed;
            right: 0;
            bottom: 0;
            transform: translateY(-8pt);
            white-space: nowrap;
        }}  
        @bottom-left {{
            content: "ROBERT v {robert_version}";
            font-size: 8pt;
            position: fixed;
            left: 0;
            bottom: 0;
            transform: translateY(-8pt);
            white-space: nowrap;
        }}
        @bottom-center {{
            content: "";
            border-top: 3px solid black;
            width: 100%;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(8pt);
        }}  
        @top-center {{
            content: "";
            border-top: 3px solid black;
            width: 100%;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(40pt);
        }}  
        @top-left {{
            content: "ROBERT Report";
            font-size: 8pt;
            font-weight:bold;
            position: fixed;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(2pt);
            white-space: nowrap;
        }} 
        @top-right {{
            content: "{csv_name}";
            font-size: 8pt;
            font-style: italic;
            position: fixed;
            position: fixed;
            left: 0;
            right: 0;
            bottom: 0pt;
            transform: translateY(2pt);
            white-space: nowrap;
        }} 
    }}
    * {{
        font-family: "Helvetica", Arial, sans-serif;
    }}
    .dat-content {{
        width: 50%;
        max-width: 595pt;
        overflow-x: auto;
        line-height: 1.2;
    }}

    img[src="Robert_logo.jpg"] {{
        float: center;
    }}
    img[src*="Pearson"] {{
        display: inline-block;
        vertical-align: bottom;
        max-width: 48%;
        margin-left: 10px;
        margin-bottom: -5px;
    }}
    img[src*="PFI"] {{
        display: inline-block;
        vertical-align: bottom;
        max-width: 48%;
        margin-left: 10px;
        margin-bottom: -5px;
    }}

    img[src*="PFI"]:first-child {{
        margin-right: 10%;
    }}
    .img-PREDICT {{
        margin-top: 20px;
    }}
    
    hr.black {{
    border: none;
    height: 3px;
    background-color: black;
    }}
    
    hr {{
    border: none;
    height: 1px;
    background-color: gray;
    }}

    body:before {{
    top: 1.2cm;
    }}
    """
    return css_content


def format_lines(module_data, max_width=122):
    """
    Reads a file and returns a formatted string between two markers
    """

    formatted_lines = []
    lines = module_data.split('\n')
    for i,line in enumerate(lines):
        if 'R2' in line:
            line = line.replace('R2','R<sup>2</sup>')
        formatted_line = textwrap.fill(line, width=max_width, subsequent_indent='')
        if i > 0:
            formatted_lines.append(f'<pre style="text-align: justify;">\n{formatted_line}</pre>')
        else:
            formatted_lines.append(f'<pre style="text-align: justify;">{formatted_line}</pre>\n')

    return ''.join(formatted_lines)


def get_y_values(file,y_param):
    """
    Gets y and y_pred values from the PREDICT folder
    """

    df = pd.read_csv(file)

    y_val = df[y_param]
    y_val_pred = df[f'{y_param}_pred']

    return y_val,y_val_pred
